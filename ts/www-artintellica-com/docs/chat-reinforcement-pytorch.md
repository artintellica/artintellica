+++
model = "gpt-4.1"
+++

i need your help with a project. i am learning machine learning, and i am doing it in a unique way. i am writing an open source blog called Artintellica. i have already gone through linear algebra, calculus, and deep learning. i am now moving onto reinforcement learning.

the way it works is that i am creating a series of blog posts for each subject. each blog post has math, python examples, and exercises in python. although some subjects are very mathematicaly, we never have purely math exercises. instead, we always have python code exercises.

the next thing i am doing is reinforcement learning. the basic idea for this course is to learn reinforcement learning with pytorch. we will be starting with elemtnary matrices and vectors, and then moving onto more advanced subjects including neural networks, reinforcement learning, and deep reinforcement learning.

i need your help to product the first blog post. each blog post should always start with front-matter, which looks like this:

+++
title = "Learn Reinforcement Learning with PyTorch, Part N.M: [the title of the blog post]"
author = "Artintellica"
date = "2025-06-09" [or whatever the date is]
+++

for each blog post, always give an explanation of the subject at hand. whenever necessary, use mathematics, including definitions and perhaps occassionally mathematical proofs. be sure to include introductory and concluding sections. the basic structure of each blog post should be something like this:

- introduction
- mathematics (if necessary)
- python demonstrations with full code
- python exercises for the reader to try
- conclusion

for all python code, always use python type hints wherever possible. and if you need to give installation instructions, use uv.

now i will proceed to give you the outline. can you please respond by writing the first blog post? please respond with the blog post only. oh, and be sure that for all math you do, you always do dollar-denominated math. use one dollar symbol for inline math, and two dollar symbols for display math.

the outline is as follows:

---

# **Course Outline: Learn Reinforcement Learning with PyTorch**

---

## **Module 1: Foundations—Vectors, Matrices, and PyTorch Basics**

### 1. Introduction: The Course, the Blog, and Why PyTorch

* **Exercise 1:** Install PyTorch and verify the installation.
* **Exercise 2:** Create your first tensor and print its shape and dtype.
* **Exercise 3:** Move a tensor between CPU and (if available) GPU/MPS, and print device info.
* **Exercise 4:** Run and explain a “Hello World” example—elementwise addition in PyTorch.

### 2. Vectors and Scalars—Hands-On with PyTorch Tensors

* **Exercise 1:** Create scalar and 1D vector tensors; change their shape.
* **Exercise 2:** Index and slice vectors; reverse a vector.
* **Exercise 3:** Compute mean, sum, and standard deviation of a vector.
* **Exercise 4:** Plot a vector as a line graph using Matplotlib.

### 3. Basic Vector Operations: Addition, Scalar Multiplication, Dot Product

* **Exercise 1:** Implement vector addition using PyTorch.
* **Exercise 2:** Scale a vector by a scalar (both manual and using broadcasting).
* **Exercise 3:** Compute the dot product of two vectors (manual and built-in).
* **Exercise 4:** Visualize vector addition and scalar multiplication in 2D with Matplotlib.

### 4. Matrices: Construction, Shapes, and Basic Matrix Operations

* **Exercise 1:** Create 2D tensors (matrices) with specific shapes.
* **Exercise 2:** Transpose a matrix and verify with PyTorch.
* **Exercise 3:** Perform elementwise addition and multiplication on two matrices.
* **Exercise 4:** Demonstrate broadcasting with a matrix and a vector.

### 5. Broadcasting and Elementwise Operations in PyTorch

* **Exercise 1:** Add a row vector to each row of a matrix using broadcasting.
* **Exercise 2:** Multiply a matrix by a column vector using broadcasting.
* **Exercise 3:** Identify and fix a broadcasting error in code.
* **Exercise 4:** Compare manual elementwise operations with PyTorch’s built-in operators.

### 6. Matrix Multiplication and Transpose—What, Why, and How

* **Exercise 1:** Multiply two matrices using `@` and `torch.matmul`.
* **Exercise 2:** Implement matrix multiplication “by hand” using loops and compare with PyTorch.
* **Exercise 3:** Visualize the effect of transposing a matrix on its shape and data.
* **Exercise 4:** Explain and fix a common shape-mismatch error in matmul.

### 7. Geometry with Tensors: Norms, Distance, Angles, Projections

* **Exercise 1:** Compute the Euclidean norm (length) of a vector.
* **Exercise 2:** Find the distance between two vectors.
* **Exercise 3:** Calculate the cosine similarity between two vectors.
* **Exercise 4:** Project one vector onto another and plot both.

### 8. Linear Transformations and Simple Data Transformations

* **Exercise 1:** Create rotation and scaling matrices.
* **Exercise 2:** Apply a rotation matrix to a set of 2D points.
* **Exercise 3:** Visualize the effect of a transformation on a shape (e.g., a square).
* **Exercise 4:** Chain multiple transformations and describe the result.

### 9. Hands-On Mini-Project: Visualizing and Transforming Data with Tensors

* **Exercise 1:** Load a simple 2D dataset (or generate synthetic data).
* **Exercise 2:** Apply various matrix transformations and visualize before/after.
* **Exercise 3:** Compute the mean and covariance matrix of the dataset.
* **Exercise 4:** Center and normalize the data with PyTorch.

---

## **Module 2: Optimization and Learning—From Gradients to Linear Models**

### 1. Introduction to Gradient Descent—Math and Code

* **Exercise 1:** Implement scalar gradient descent for a simple function (e.g., \$f(x) = x^2\$).
* **Exercise 2:** Visualize the optimization path on a 2D plot.
* **Exercise 3:** Use vector gradient descent on \$f(\mathbf{x}) = ||\mathbf{x}||^2\$.
* **Exercise 4:** Experiment with different learning rates and observe convergence.

### 2. Autograd in PyTorch: Automatic Differentiation Demystified

* **Exercise 1:** Mark a tensor as `requires_grad=True` and compute gradients.
* **Exercise 2:** Calculate gradients for a multi-variable function (e.g., \$f(x, y) = x^2 + y^3\$).
* **Exercise 3:** Zero gradients and explain why this is necessary in training loops.
* **Exercise 4:** Manually verify PyTorch’s computed gradients for a small example.

### 3. Loss Functions and Cost Surfaces—Visual and Practical Intuition

* **Exercise 1:** Implement Mean Squared Error (MSE) loss manually and with PyTorch.
* **Exercise 2:** Plot the loss surface for a simple linear model.
* **Exercise 3:** Visualize binary cross-entropy loss as a function of its input.
* **Exercise 4:** Compare how different loss functions penalize outliers.

### 4. Fitting a Line: Linear Regression from Scratch with PyTorch

* **Exercise 1:** Generate synthetic linear data with noise.
* **Exercise 2:** Implement linear regression training loop from scratch (using only tensors).
* **Exercise 3:** Use PyTorch’s autograd and optimizer to fit a line.
* **Exercise 4:** Plot predictions vs. ground truth and compute R² score.

### 5. Fitting Nonlinear Curves: Polynomial Regression

* **Exercise 1:** Generate polynomial data with noise.
* **Exercise 2:** Fit a polynomial using explicit feature construction in PyTorch.
* **Exercise 3:** Compare performance of linear vs. polynomial regression.
* **Exercise 4:** Visualize overfitting by increasing the polynomial degree.

### 6. Classification Basics: Logistic Regression

* **Exercise 1:** Generate binary classification data.
* **Exercise 2:** Implement logistic regression “from scratch” (sigmoid and BCE loss).
* **Exercise 3:** Train with PyTorch’s optimizer and compare results.
* **Exercise 4:** Plot decision boundary and accuracy.

### 7. Softmax and Multiclass Classification

* **Exercise 1:** Generate synthetic data for three classes.
* **Exercise 2:** Implement softmax and cross-entropy loss manually.
* **Exercise 3:** Train a multiclass classifier with PyTorch’s `nn.CrossEntropyLoss`.
* **Exercise 4:** Plot the class boundaries in 2D.

### 8. Mini-Project: Build, Train, and Visualize a Simple Classifier

* **Exercise 1:** Choose or generate a 2D dataset for classification.
* **Exercise 2:** Train a classifier (logistic or neural network) on the data.
* **Exercise 3:** Visualize the learned decision boundary.
* **Exercise 4:** Evaluate model accuracy and plot misclassified points.

---

## **Module 3: Neural Networks—Building Blocks and Training**

### 1. The Perceptron: Oldest Neural Network

* **Exercise 1:** Implement the perceptron update rule for a binary task.
* **Exercise 2:** Train the perceptron on linearly separable data.
* **Exercise 3:** Visualize decision boundary after each epoch.
* **Exercise 4:** Test on non-separable data and analyze behavior.

### 2. Feedforward Neural Networks from Scratch (No nn.Module)

* **Exercise 1:** Implement a two-layer neural network using tensors and matrix ops.
* **Exercise 2:** Forward propagate and compute loss for a batch of inputs.
* **Exercise 3:** Backpropagate gradients manually (by hand or using PyTorch’s autograd).
* **Exercise 4:** Train the model and plot loss over epochs.

### 3. Building with `torch.nn`: The Convenient Way

* **Exercise 1:** Rewrite previous NN using `nn.Module`.
* **Exercise 2:** Compare number of lines and readability.
* **Exercise 3:** Add a hidden layer and train on data.
* **Exercise 4:** Save and load model weights.

### 4. Activation Functions: Sigmoid, Tanh, ReLU, LeakyReLU, etc.

* **Exercise 1:** Plot different activation functions.
* **Exercise 2:** Train small NNs with each activation on the same task; compare convergence.
* **Exercise 3:** Observe vanishing/exploding gradients by visualizing gradients.
* **Exercise 4:** Swap activation mid-training and observe changes.

### 5. Backpropagation: Intuition and Hands-On Example

* **Exercise 1:** Compute gradients for a two-layer network by hand for a single example.
* **Exercise 2:** Use `.backward()` to compare with manual gradients.
* **Exercise 3:** Visualize gradient flow in the network.
* **Exercise 4:** Debug and fix a model with vanishing gradients.

### 6. Overfitting, Underfitting, and Regularization

* **Exercise 1:** Fit a model to small and large datasets and plot train/test loss.
* **Exercise 2:** Add noise to data and visualize overfitting.
* **Exercise 3:** Apply L2 regularization and observe effect.
* **Exercise 4:** Vary model complexity and record accuracy.

### 7. Dropout, L2, and Other Regularization in PyTorch

* **Exercise 1:** Add dropout layers to a network and plot training/validation accuracy.
* **Exercise 2:** Use weight decay with an optimizer.
* **Exercise 3:** Compare models with and without regularization.
* **Exercise 4:** Experiment with different dropout rates and interpret results.

### 8. Mini-Project: MNIST Digit Classifier (Shallow NN)

* **Exercise 1:** Download and load the MNIST dataset using PyTorch.
* **Exercise 2:** Build and train a shallow NN on MNIST.
* **Exercise 3:** Plot confusion matrix of predictions.
* **Exercise 4:** Visualize a few correctly and incorrectly classified digits.

---

## **Module 4: Introduction to Reinforcement Learning—Concepts & Classic Problems**

### 1. What is Reinforcement Learning? RL vs. Supervised/Unsupervised

* **Exercise 1:** Simulate an agent-environment loop and print state transitions.
* **Exercise 2:** Assign rewards to agent actions in a toy environment.
* **Exercise 3:** Compare supervised, unsupervised, and RL code snippets.
* **Exercise 4:** Plot agent total reward over time for a simple policy.

### 2. Markov Decision Processes: States, Actions, Rewards, Policies

* **Exercise 1:** Define a small MDP as Python data structures.
* **Exercise 2:** Simulate a random policy in the MDP and track rewards.
* **Exercise 3:** Visualize state transitions as a graph.
* **Exercise 4:** Implement a simple policy and compute expected reward.

### 3. Bandit Problems: Exploration vs. Exploitation

* **Exercise 1:** Simulate a multi-armed bandit and plot arm payout distributions.
* **Exercise 2:** Implement epsilon-greedy policy and track arm selections.
* **Exercise 3:** Compare UCB and Thompson Sampling on the same problem.
* **Exercise 4:** Plot cumulative reward for different strategies.

  * *Project Exercise:* Visualize learning curve of a bandit agent over time.

### 4. Tabular Value-Based Methods: Q-Learning and SARSA

* **Exercise 1:** Implement a Q-table for a small discrete environment.
* **Exercise 2:** Run Q-learning and SARSA on Gridworld, compare convergence.
* **Exercise 3:** Visualize the learned Q-table as a heatmap.
* **Exercise 4:** Animate agent’s trajectory using the learned policy.

  * *Project Exercise:* Train a Q-learning agent on Taxi-v3 and report average episode length.

### 5. Monte Carlo and Temporal Difference (TD) Learning

* **Exercise 1:** Implement MC and TD(0) value updates for a toy MDP.
* **Exercise 2:** Track value estimates over episodes and plot.
* **Exercise 3:** Compare convergence of MC and TD(0).
* **Exercise 4:** Use value estimates to improve policy and evaluate performance.

  * *Project Exercise:* Visualize value function estimates on FrozenLake-v1.

### 6. Policies, Value Functions, and Bellman Equations

* **Exercise 1:** Solve a small Bellman equation system by hand and with PyTorch.
* **Exercise 2:** Compute state-value and action-value functions for a policy.
* **Exercise 3:** Evaluate a random and a greedy policy in an MDP.
* **Exercise 4:** Visualize policy and value function in a gridworld.

### 7. Mini-Project: RL Agent on Custom Gridworld

* **Exercise 1:** Design a custom Gridworld environment in Python.
* **Exercise 2:** Implement tabular Q-learning for your environment.
* **Exercise 3:** Train agent and visualize policy after each episode.
* **Exercise 4:** Modify reward structure and observe changes in learned policy.

---

## **Module 5: Deep Reinforcement Learning—From Q-Tables to Deep Networks**

### 1. Limitations of Tabular RL and the Need for Function Approximation

* **Exercise 1:** Create a large state space where Q-tables are impractical.
* **Exercise 2:** Visualize exponential state growth in a simple environment.
* **Exercise 3:** Attempt tabular learning and analyze why it fails.
* **Exercise 4:** Propose a neural network architecture to replace Q-table.

### 2. Deep Q-Networks (DQN): Concepts and PyTorch Implementation

* **Exercise 1:** Build a Q-network using `nn.Module` for CartPole-v1.
* **Exercise 2:** Implement an experience replay buffer from scratch.
* **Exercise 3:** Train DQN agent and plot episode rewards.
* **Exercise 4:** Save and load trained DQN models.

  * *Project Exercise:* Experiment with different network sizes and plot learning curves.

### 3. Experience Replay and Target Networks

* **Exercise 1:** Implement a target network for stable Q-value updates.
* **Exercise 2:** Compare learning with and without experience replay.
* **Exercise 3:** Tune replay buffer size and target update frequency.
* **Exercise 4:** Analyze training stability through reward variance plots.

### 4. Extensions: Double DQN, Dueling DQN

* **Exercise 1:** Extend your DQN to Double DQN and compare performance.
* **Exercise 2:** Implement dueling network architecture in PyTorch.
* **Exercise 3:** Train both models on LunarLander-v2 and compare results.
* **Exercise 4:** Visualize Q-value distributions during training.

  * *Project Exercise:* Plot side-by-side learning curves for DQN, Double DQN, and Dueling DQN.

### 5. Policy Gradient Methods: REINFORCE Algorithm

* **Exercise 1:** Implement a policy network for CartPole-v1.
* **Exercise 2:** Code the REINFORCE algorithm and train agent.
* **Exercise 3:** Visualize total reward per episode and smoothed average.
* **Exercise 4:** Experiment with reward discounting and report effects.

  * *Project Exercise:* Show learned policy in action using environment render.

### 6. Advantage Actor-Critic (A2C/A3C) - Concepts & Simple Implementation

* **Exercise 1:** Implement separate actor and critic networks in PyTorch.
* **Exercise 2:** Train A2C agent on CartPole-v1.
* **Exercise 3:** Plot both value loss and policy loss over training.
* **Exercise 4:** Visualize the advantage estimates during episodes.

# === ASSISTANT ===

+++
title = "Learn Reinforcement Learning with PyTorch, Part 1.1: Introduction—The Course, the Blog, and Why PyTorch"
author = "Artintellica"
date = "2024-06-09"
+++

Welcome! This is the first entry in **Artintellica's Reinforcement Learning series**—a practical, code-driven open-source course where we build mathematical foundations and RL agents **from vectors to deep Q-networks, all with PyTorch**. Whether you've found us via GitHub, search, or your love of math and code, *thank you*. This blog is intended for ambitious beginners who want more than a "black box" approach.

What sets this course apart?

- **Code-first:** Every exercise uses Python, and everything is done in code—even math.
- **Open source:** Every post, example, and dataset is MIT licensed. Fork, modify, use!
- **Math-backed:** We'll use mathematics where appropriate—but always paired with concrete, hands-on coding.
- **PyTorch-powered:** PyTorch is now the dominant RL and deep learning research tool, with a gentle learning curve, intuitive tensor ops, and blazing performance on CPUs and GPUs.

**This first post** serves as both an orientation and your first hands-on steps: installing PyTorch, running "Hello Tensors", and manipulating your first data structures.

---

## What Will You Learn?

This post will help you:

- Understand how this RL course is structured (and what comes next)
- Install PyTorch on your system using the `uv` package manager
- Create and manipulate your first `torch.Tensor` objects
- Move tensors between CPU and GPU (if available)
- Run and interpret your first PyTorch computation

Each concept comes with code and hands-on exercises. By the end, you'll be ready for the math and code engine that powers everything in modern deep learning (and, soon, RL!).

---

## Why PyTorch? Why Not Just Numpy?

PyTorch is a numeric computation library like NumPy, but with several important differences that make it essential for modern ML and RL:

1. **Automatic Differentiation:** Critical for optimization and training models.
2. **Hardware Acceleration:** Fast computation on CPUs, GPUs (CUDA), and Apple Silicon (MPS) with almost no code changes.
3. **Dynamic Graphs:** Models can be defined and modified "on the fly", perfect for research and RL.
4. **Neural Network Utilities:** Everything from layers to RL-specific frameworks.
5. **Community & Ecosystem:** Used everywhere—if you can do it in deep RL, you can probably do it in PyTorch.

Throughout this course, you'll see how PyTorch lets us build RL agents—from table-based to neural-based—with minimal friction.

---

## Mathematics: How PyTorch Relates to Vectors and Matrices

At its core, PyTorch's **Tensor** object generalizes the matrix and vector concepts from linear algebra. In this blog, everything you learned about scalars ($a$), vectors ($\mathbf{x}$), and matrices ($A$) will correspond directly to PyTorch `Tensor` objects—uniquely enabling you to:

- Write code that *is* the math
- Run experiments interactively
- Train and debug agents at every complexity level

We'll often use math notation ($\mathbf{x}$, $A$, etc.) side-by-side with its exact code representation (`torch.Tensor`). As you encounter concepts like dot product, matrix multiplication, or vector norms, we'll show both the formula and its PyTorch incarnation.

---

## Installing PyTorch (with `uv`)

> ℹ️ For this blog, we'll assume you use Python 3.9 or later on Linux, macOS, or Windows. Use VS Code, Jupyter, or any Python IDE you like.

We recommend [uv](https://github.com/astral-sh/uv) for fast, reproducible, modern package management.

### **Step 1:** Install `uv` (if not already installed)

```sh
pip install --upgrade uv
```

### **Step 2:** Create a new project directory and initialize

```sh
mkdir rl-with-pytorch && cd rl-with-pytorch
uv venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
```

### **Step 3:** Install PyTorch + matplotlib (for plots)

Find your install command at: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).

For most CPU-only systems:

```sh
uv pip install torch torchvision torchaudio matplotlib
```

For CUDA-enabled GPUs, use the correct `torch` version:

```sh
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
uv pip install matplotlib
```

### **Verify Installation**

Open Python:

```python
import torch
print("Torch version:", torch.__version__)
print(torch.rand(2, 2))
```

If this prints the version and a $2\times 2$ tensor of random numbers, you're ready!

---

## Python Demonstrations

Let's walk through key tensor operations.

### **Demo 1: Create a Tensor and Print Its Data, Shape, dtype**

```python
import torch

# Create a 1D tensor of floats, from 0 to 4
x: torch.Tensor = torch.arange(5, dtype=torch.float32)
print("x:", x)  # Tensor data
print("Shape:", x.shape)
print("Dtype:", x.dtype)
```

### **Demo 2: Move Tensor to GPU (if available) or MPS (Apple Silicon)**

```python
# Detect GPUs or MPS device (Apple Silicon)
if torch.cuda.is_available():
    device: torch.device = torch.device("cuda")
elif getattr(torch, "has_mps", False) and torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")
print(f"Using device: {device}")

x_gpu: torch.Tensor = x.to(device)
print("x is on device:", x_gpu.device)
```

### **Demo 3: "Hello World"—Elementwise Addition**

Elementwise addition is just like with math or NumPy: $z_i = x_i + y_i$

```python
y: torch.Tensor = torch.ones(5, dtype=torch.float32, device=device)
z: torch.Tensor = x_gpu + y  # elementwise add
print("z:", z)
```

---

## Exercises

Try these yourself! (Start from a new Python file, or use a Jupyter notebook.)

### **Exercise 1:** Install PyTorch and Verify

- Install using `uv` as above. Write Python code to import `torch` and print its version.

### **Exercise 2:** Create Your First Tensor

- Create a tensor of shape `(4, 3)` (4 rows, 3 cols) filled with random numbers.
- Print the tensor, its `shape`, and `dtype`.

### **Exercise 3:** Move Tensor Between Devices

- Detect if a GPU or MPS device is available.
- Move a tensor to CPU, then to device, and print its `.device` at each stage.

### **Exercise 4:** Hello World Addition

- Create two tensors of size `(6,)` (one with all zeros, one with all ones).
- Add them and print the result.

---

**Sample Starter Code for Exercises:**

```python
import torch

# EXERCISE 1
print("PyTorch version:", torch.__version__)

# EXERCISE 2
a: torch.Tensor = torch.randn(4, 3)
print("Tensor a:", a)
print("Shape:", a.shape)
print("Dtype:", a.dtype)

# EXERCISE 3
device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
b: torch.Tensor = torch.arange(6)
print("Before:", b.device)
b = b.to(device)
print("After:", b.device)

# EXERCISE 4
zero: torch.Tensor = torch.zeros(6)
one: torch.Tensor = torch.ones(6)
sum_: torch.Tensor = zero + one
print("Sum:", sum_)
```

---

## Conclusion

In this post, you've:

- Oriented yourself to the **Artintellica RL with PyTorch course**
- Learned how to quickly set up a PyTorch environment with `uv`
- Explored PyTorch's powerful tensor abstraction as a bridge between math and code
- Written your first PyTorch code for tensor creation, device moves, and addition
- Practiced with beginner exercises you'll build on throughout the course

**Next:** We'll dig into the fundamentals of vectors and scalars in PyTorch, learning to manipulate, index, and visualize them—the real foundation for RL algorithms.

# === USER ===

that was great, i have finished going through the first blog post. now, can you please write the second blog post? the description is:

### 2. Vectors and Scalars—Hands-On with PyTorch Tensors

* **Exercise 1:** Create scalar and 1D vector tensors; change their shape.
* **Exercise 2:** Index and slice vectors; reverse a vector.
* **Exercise 3:** Compute mean, sum, and standard deviation of a vector.
* **Exercise 4:** Plot a vector as a line graph using Matplotlib.


# === ASSISTANT ===

+++
title = "Learn Reinforcement Learning with PyTorch, Part 1.2: Vectors and Scalars—Hands-On with PyTorch Tensors"
author = "Artintellica"
date = "2024-06-09"
+++

## Introduction

Welcome back to Artintellica’s open-source RL course! After setting up PyTorch and running your first tensor operations, it’s time to ground ourselves in the basics: **vectors and scalars**. Tensors are the backbone of all computations in PyTorch and reinforcement learning. Understanding how to create, reshape, and manipulate these 1D (and 0D) structures is crucial whether you’re implementing bandits, processing observations, or building neural networks.

In this post, we’ll:

- See how **scalars** (single numbers) and **vectors** (1D arrays) arise in both mathematics and PyTorch.
- Learn how to create and reshape tensors, index and slice them, and compute basic statistics.
- Visualize vectors as sequences—an essential skill for debugging neural networks and RL environments.
- Provide hands-on coding exercises so you have full mastery before moving on to matrices (next time).

Let’s get started!

---

## Mathematics: Scalars and Vectors

Mathematically,

- A **scalar** is a single number, $a \in \mathbb{R}$ (for example, the reward at a particular step).
- A **vector** is a one-dimensional array of numbers, $\mathbf{x} = [x_1, x_2, \ldots, x_n]^\top$, where $n$ is the number of elements (for example, an agent’s observation or a parameter vector).

In PyTorch:

- **Scalar tensor:** A tensor of zero dimensions (`torch.tensor(3.0)`)
- **Vector tensor:** A tensor with shape `(n,)` for $n$ elements, i.e., `torch.tensor([1.0, 2.0, 3.0])`

PyTorch allows you to reshape, index, slice, and analyze these with almost NumPy-like power—plus GPU acceleration.

---

## Demonstrations: Cooking with Scalars and Vectors

### Creating and Reshaping Scalars & Vectors

```python
import torch

# Scalar (0-dimensional tensor, holds a single value)
a: torch.Tensor = torch.tensor(7.5)
print("Scalar a:", a)
print("Shape (should be torch.Size([])):", a.shape)
print("Dimensions (should be 0):", a.dim())

# 1-D vector tensor
v: torch.Tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])
print("Vector v:", v)
print("Shape:", v.shape)
print("Dimensions:", v.dim())

# Changing shape: Reshape vector v to a column vector (n, 1)
v_col: torch.Tensor = v.unsqueeze(1)  # adds a new dimension at position 1
print("v as column vector (shape):", v_col.shape)

# Flatten v_col back to 1D
v_flat: torch.Tensor = v_col.squeeze()
print("v_col squeezed to 1D:", v_flat.shape)
```

### Indexing, Slicing, and Reversing

```python
# Indexing: Get the second element (index 1)
second: torch.Tensor = v[1]
print("Second element:", second.item())  # .item() to extract Python float

# Slicing: Get the first three elements
first_three: torch.Tensor = v[:3]
print("First three elements:", first_three)

# Reversing the vector
reversed_v: torch.Tensor = v.flip(0)
print("Reversed v:", reversed_v)
```

### Basic Statistics: Mean, Sum, Std

```python
# Compute statistics
mean_v: torch.Tensor = v.mean()
sum_v: torch.Tensor = v.sum()
std_v: torch.Tensor = v.std(unbiased=False)  # Match numpy's normalization

print(f"Mean: {mean_v.item():.2f}")
print(f"Sum: {sum_v.item():.2f}")
print(f"Std: {std_v.item():.2f}")
```

### Plotting Vectors with Matplotlib

For visualization (so important in RL diagnostics!), let’s plot a vector as a line graph.

```python
import matplotlib.pyplot as plt

# Let's create a vector of sine values
t: torch.Tensor = torch.linspace(0, 2 * torch.pi, 100)
sin_t: torch.Tensor = torch.sin(t)

plt.figure(figsize=(8, 4))
plt.plot(t.numpy(), sin_t.numpy())
plt.title("Sine Wave")
plt.xlabel("t")
plt.ylabel("sin(t)")
plt.grid(True)
plt.show()
```

---

## Exercises

#### **Exercise 1: Create Scalar and 1D Vector Tensors; Change Their Shape**

- Create a scalar tensor with the value $42$.
- Create a vector tensor with values $[3, 1, 4, 1, 5, 9]$ as floats.
- Reshape your vector to a **column** (shape $(6, 1)$) and then back to **row** (shape $(6,)$).

#### **Exercise 2: Index, Slice, and Reverse Vectors**

- Print the first and last element of your vector.
- Slice out every other element (e.g., elements at even indices).
- Reverse your vector using PyTorch.

#### **Exercise 3: Compute Mean, Sum, and Standard Deviation**

- Compute and print the mean, sum, and standard deviation of your vector, with each rounded to two decimals.

#### **Exercise 4: Plot a Vector as a Line Graph Using Matplotlib**

- Create a PyTorch vector containing $100$ linearly spaced points between $0$ and $4\pi$.
- Compute the cosine of each value.
- Use matplotlib to plot the vector as a line graph.

---

### **Sample Starter Code for Exercises**

```python
import torch
import matplotlib.pyplot as plt

# EXERCISE 1
scalar: torch.Tensor = torch.tensor(42.0)
vec: torch.Tensor = torch.tensor([3, 1, 4, 1, 5, 9], dtype=torch.float32)
vec_col: torch.Tensor = vec.unsqueeze(1)
vec_row: torch.Tensor = vec_col.squeeze()
print("Scalar:", scalar)
print("Vector:", vec)
print("As column shape:", vec_col.shape)
print("Back to row shape:", vec_row.shape)

# EXERCISE 2
print("First element:", vec[0].item())
print("Last element:", vec[-1].item())
print("Every other element:", vec[::2])
print("Reversed:", vec.flip(0))

# EXERCISE 3
print("Mean: {:.2f}".format(vec.mean().item()))
print("Sum: {:.2f}".format(vec.sum().item()))
print("Std: {:.2f}".format(vec.std(unbiased=False).item()))

# EXERCISE 4
t: torch.Tensor = torch.linspace(0, 4 * torch.pi, 100)
cos_t: torch.Tensor = torch.cos(t)
plt.plot(t.numpy(), cos_t.numpy())
plt.title(r"$\cos(t)$ from $t=0$ to $4\pi$")
plt.xlabel("t")
plt.ylabel("cos(t)")
plt.grid(True)
plt.show()
```

---

## Conclusion

Today, you’ve gained hands-on experience with the two most fundamental building blocks in PyTorch and RL—scalars and vectors. You learned to create, reshape, index, reverse, compute statistics, and even plot them. Mastery of these basics makes everything to come—matrix math, neural networks, and reinforcement learning algorithms—much easier and more intuitive.

**In the next post**, we will extend these skills to *vector operations*: addition, scaling, and dot products. Being comfortable with tensors, dimensions, and slicing will let you focus on building and debugging RL agents instead of being tripped up by tensor shapes!

*Keep practicing, play with your own vectors, and see you in Part 1.3!*

# === USER ===


