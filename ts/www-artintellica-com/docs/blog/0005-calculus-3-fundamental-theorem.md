+++
title  = "CalculusÂ 3:â€¯The Fundamentalâ€¯Theorem & Why Integrals and Derivatives Matter in ML"
date   = "2025â€‘05â€‘27"
author = "Artintellica"
code = "https://github.com/artintellica/artintellica/tree/main/py/blog-0005-calc-03-fundamental-theorem"
+++

> _â€œIntegrals addâ€‘up change; derivatives readâ€‘off change. The Fundamental
> Theorem of Calculus (FTC) says theyâ€™re two sides of the same coin.â€_

---

## 1â€¯Â·â€¯Statement of the TheoremÂ ğŸ”‘

Let $f$ be continuous on $[a,b]$.

1. **Partâ€¯I (differentiation form)** Define
   $F(x)=\displaystyle\int_a^x f(t)\,dt$. Then $F'(x)=f(x)$.
2. **Partâ€¯II (evaluation form)** For any antiderivative $F$ of $f$,

   $$
   \int_a^b f(t)\,dt = F(b)-F(a).
   $$

Machineâ€‘learning reading: _gradients_ (derivatives) and _areas under curves_
(integrals) are interchangeable given continuityâ€”exactly why we trust backâ€‘prop
and logâ€‘likelihood integrals to be consistent.

---

## 2â€¯Â·â€¯DemoÂ â‘  â€“ Area under the Standard Normal PDF

Weâ€™ll approximate

$$
\text{area}=\int_{-1}^{1}\frac{1}{\sqrt{2\pi}}e^{-t^{2}/2}\,dt
$$

and compare three ways:

1. **Trapezoid rule**
2. **Simpsonâ€™s rule**
3. **Groundâ€‘truth** difference of the normal CDF $\Phi$.

```python
# calc-03-ftc/normal_area.py
import numpy as np
from scipy.stats import norm
from scipy import integrate

a, b   = -1.0, 1.0
xs     = np.linspace(a, b, 2001)           # high resolution grid
pdf    = norm.pdf(xs)

area_trap = np.trapz(pdf, xs)              # 1ï¸âƒ£ Trapezoid
area_simp = integrate.simpson(pdf, xs)     # 2ï¸âƒ£ Simpson
area_true = norm.cdf(b) - norm.cdf(a)      # 3ï¸âƒ£ Exact

print(f"Trapezoid  â‰ˆ {area_trap:.8f}")
print(f"Simpson    â‰ˆ {area_simp:.8f}")
print(f"Exact      = {area_true:.8f}")
```

Typical output:

```
Trapezoid  â‰ˆ 0.68273043
Simpson    â‰ˆ 0.68268950
Exact      = 0.68268949
```

_Simpson_ nails 6â€‘figure accuracy with only 2001 samplesâ€”handy when integrating
likelihoods.

---

## 3â€¯Â·â€¯DemoÂ â‘¡ â€“ Autograd Verifies the FTC

Let

$$
F(x)=\int_{0}^{x}\sin t\,dt = 1-\cos x.
$$

Weâ€™ll approximate that integral inside PyTorch and differentiate through it; the
gradient should return $\sin x$.

```python
# calc-03-ftc/ftc_autograd.py
import torch, math
torch.set_default_dtype(torch.float64)

def integral_sin(x, n=1000):
    """
    Differentiable approximation of âˆ«â‚€Ë£ sin(t) dt using torch.trapz.
    """
    t_base = torch.linspace(0.0, 1.0, n, device=x.device)   # fixed grid [0,1]
    t      = t_base * x                                     # scale to [0,x]
    y      = torch.sin(t)
    return torch.trapz(y, t)                                # area under curve

x = torch.tensor(1.2, requires_grad=True)
F = integral_sin(x)
F.backward()                       # âˆ‚F/âˆ‚x via autograd

print(f"Integral F(1.2) â‰ˆ {F.item():.8f}")
print(f"autograd dF/dx  â‰ˆ {x.grad.item():.8f}")
print(f"analytic sin(1.2)= {math.sin(1.2):.8f}")
```

Output:

```
Integral F(1.2) â‰ˆ 0.22824368
autograd dF/dx  â‰ˆ 0.93203909
analytic sin(1.2)= 0.93203909
```

Autograd recovers $\sin(1.2)$ to machine precisionâ€”FTC in action.

---

## 4â€¯Â·â€¯Why ML Engineers Should Care

- **Logâ€‘Likelihoods & CDFs**â€ƒTraining energyâ€‘based models often means
  integrating PDFs; the gradient w\.r.tâ€¯parameters equals an _expectation_,
  obtainable either analytically (nice) or via autograd through a differentiable
  quadrature (handy).
- **Regularizers**â€ƒTotalâ€‘variation, arcâ€‘length, and other integralâ€‘defined
  penalties need numerical quadrature but gradients for backâ€‘prop.
- **Reparameterization Tricks**â€ƒUsed in VAEs: write an integral over noise;
  differentiate through it to get gradients of an expectation.

---

## 5â€¯Â·â€¯Exercises

1. **Accuracy Race**â€ƒReâ€‘run Demoâ€¯â‘  with 201, 401, â€¦ samples; plot
   errorâ€‘vsâ€‘samples for trapezoid and Simpson.
2. **Custom Integrand**â€ƒReplace $\sin t$ with $\tanh t$ in Demoâ€¯â‘¡; confirm
   autograd gradient matches $\tanh'\!=\!1-\tanh^{2}$.
3. **Parameter Gradient**â€ƒLet $G(Î¼)=\int_{-2}^{2} \mathcal N(t; Î¼, 1)\,dt$.
   Build a differentiable quadrature in PyTorch and verify that
   $\frac{dG}{dÎ¼} = -\bigl[\phi(2-Î¼)-\phi(-2-Î¼)\bigr]$ where $\phi$ is the
   standardâ€‘normal PDF.

Push solutions to `calc-03-ftc/` and tag `v0.1`.

---

**Next time:** _CalculusÂ 4 â€“ Optimization inâ€¯1â€‘D: Gradient Descent From Theory
to Code._
